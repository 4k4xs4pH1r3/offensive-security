# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# NO CHECKED-IN PROTOBUF GENCODE
# source: x.proto
# Protobuf Python Version: 5.29.1
"""Generated protocol buffer code."""

from google.api import annotations_pb2 as google_dot_api_dot_annotations__pb2
from google.api import client_pb2 as google_dot_api_dot_client__pb2
from google.api import field_behavior_pb2 as google_dot_api_dot_field__behavior__pb2
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import runtime_version as _runtime_version
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder

_runtime_version.ValidateProtobufRuntimeVersion(
    _runtime_version.Domain.PUBLIC, 5, 29, 1, "", "x.proto"
)
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()

DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
    b'\n\x07x.proto\x12\x10google.ollama.v1\x1a\x1cgoogle/api/annotations.proto\x1a\x17google/api/client.proto\x1a\x1fgoogle/api/field_behavior.proto"\xc1\x01\n\x1dGenerateTextCompletionRequest\x12\x14\n\x07project\x18\x01 \x01(\tB\x03\xe0\x41\x02\x12\x12\n\x05model\x18\x02 \x01(\tB\x03\xe0\x41\x02\x12\x13\n\x06prompt\x18\x03 \x01(\tB\x03\xe0\x41\x02\x12\x12\n\nmax_tokens\x18\x04 \x01(\x05\x12\x13\n\x0btemperature\x18\x05 \x01(\x02\x12\r\n\x05top_k\x18\x06 \x01(\x05\x12\r\n\x05top_p\x18\x07 \x01(\x02\x12\x1a\n\x12repetition_penalty\x18\x08 \x01(\x02"4\n\x1eGenerateTextCompletionResponse\x12\x12\n\ncompletion\x18\x01 \x01(\t2\xe0\x01\n\x06Ollama\x12\xd5\x01\n\x16GenerateTextCompletion\x12/.google.ollama.v1.GenerateTextCompletionRequest\x1a\x30.google.ollama.v1.GenerateTextCompletionResponse"V\xda\x41\x16project, model, prompt\x82\xd3\xe4\x93\x02\x37"2/v1/projects/{project}/models/{model}:generateText:\x01*0\x01\x62\x06proto3'
)

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, "x_pb2", _globals)
if not _descriptor._USE_C_DESCRIPTORS:
    DESCRIPTOR._loaded_options = None
    _globals["_GENERATETEXTCOMPLETIONREQUEST"].fields_by_name[
        "project"
    ]._loaded_options = None
    _globals["_GENERATETEXTCOMPLETIONREQUEST"].fields_by_name[
        "project"
    ]._serialized_options = b"\340A\002"
    _globals["_GENERATETEXTCOMPLETIONREQUEST"].fields_by_name[
        "model"
    ]._loaded_options = None
    _globals["_GENERATETEXTCOMPLETIONREQUEST"].fields_by_name[
        "model"
    ]._serialized_options = b"\340A\002"
    _globals["_GENERATETEXTCOMPLETIONREQUEST"].fields_by_name[
        "prompt"
    ]._loaded_options = None
    _globals["_GENERATETEXTCOMPLETIONREQUEST"].fields_by_name[
        "prompt"
    ]._serialized_options = b"\340A\002"
    _globals["_OLLAMA"].methods_by_name["GenerateTextCompletion"]._loaded_options = None
    _globals["_OLLAMA"].methods_by_name[
        "GenerateTextCompletion"
    ]._serialized_options = b'\332A\026project, model, prompt\202\323\344\223\0027"2/v1/projects/{project}/models/{model}:generateText:\001*'
    _globals["_GENERATETEXTCOMPLETIONREQUEST"]._serialized_start = 118
    _globals["_GENERATETEXTCOMPLETIONREQUEST"]._serialized_end = 311
    _globals["_GENERATETEXTCOMPLETIONRESPONSE"]._serialized_start = 313
    _globals["_GENERATETEXTCOMPLETIONRESPONSE"]._serialized_end = 365
    _globals["_OLLAMA"]._serialized_start = 368
    _globals["_OLLAMA"]._serialized_end = 592
# @@protoc_insertion_point(module_scope)
